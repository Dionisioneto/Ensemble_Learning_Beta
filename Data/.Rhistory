if(!require(pacman)) install.packages("pacman"); library(pacman)
p_load(ggplot2,dplyr, betareg, beepr,betaboost,e1071,caret,compiler,gridExtra)
setwd("C:/Users/dioni/OneDrive - University of São Paulo/Doutorado em Estatística/2023.2/2 - Aprendizado de Máquina Estatístico/artigo_publicar/github_content/Data")
cadu = read.csv2("final_data.csv")
head(cadu)
y = cadu %>% subset(select=c(tax))
X = cadu %>% subset(select=c(ESPVIDA,FECTOT, RAZDEP,
E_ANOSESTUDO, T_ANALF18M,
T_FBBAS, T_FBFUND,
T_FBMED, T_FBSUPER,T_MED18M, T_SUPER25M,
GINI, PIND, PPOB, RDPC1, RDPCT,
THEIL, T_BANAGUA, T_DENS,
T_LIXO, T_LUZ,AGUA_ESGOTO,
T_M10A14CF, T_M15A17CF,
I_ESCOLARIDADE,  IDHM, IDHM_L, IDHM_R))
datacadu = cbind(X,y)
dim(datacadu)
## Separação entre treino e teste
set.seed(10)
# Proporção treino
prop_treino = 0.8
n_treino = round(nrow(datacadu) * prop_treino)
ind_treino = sample(1:nrow(datacadu), n_treino)
# Criar conjuntos de treino e teste
treinocadu = datacadu[ind_treino, ]
testecadu = datacadu[-ind_treino, ]
dim(treinocadu);dim(testecadu)
MSE = function(y,ypred){mean((y - ypred)^2)}
MAE = function(y,ypred){mean(abs(y - ypred))}
R2 = function(y,ypred){
num = sum((y-ypred)^2)
dem =  sum((y-mean(y))^2)
r = 1 - (num/dem)
return(r)
}
timebreg_inic <- Sys.time()
breg = betareg(tax ~.,
data=treinocadu,
link="loglog")
timebreg_end <- Sys.time()
## Tempo de treinamento
paste("Tempo de treinamento: ", timebreg_end-timebreg_inic)
ypredbreg = predict(breg,testecadu)
MAE(y=testecadu$tax,ypred=ypredbreg)
MSE(y=testecadu$tax,ypred=ypredbreg)
sqrt(MSE(y=testecadu$tax,ypred=ypredbreg))
view(datacadu)
View(datacadu)
MAE(y=testecadu$tax,ypred=ypredbreg)
MSE(y=testecadu$tax,ypred=ypredbreg)
sqrt(MSE(y=testecadu$tax,ypred=ypredbreg))
confint(breg)
set.seed(10)
kfolds = 10
modbeta = betareg(tax ~.,
data=treinocadu,
link = "loglog")
ypbreg = predict(modbeta,testecadu)
msebetareg = mean((testecadu$tax - ypbreg)^2)
msebetareg ## EQM
MSE = function(y,ypred){mean((y - ypred)^2)}
MAE = function(y,ypred){mean(abs(y - ypred))}
## RMSE
sqrt(msebetareg)
## MAE
mean(abs(testecadu$tax - ypbreg))
R2 = function(y,ypred){
num = sum((y-ypred)^2)
dem =  sum((y-mean(y))^2)
r = 1 - (num/dem)
return(r)
}
## R2: Coeficiente de Determinação
R2(y=testecadu$tax,ypred=ypbreg)
bagging_betareg = function(xtreino,ytreino,xteste,n_estimadores,n_amostra,linkfun){
## predicoes do bagging em uma matriz
matriz_bag = matrix(data=0,nrow=dim(xteste)[1],ncol=n_estimadores)
enableJIT(3)
for(preditor in 1:n_estimadores){
random_id = sample(1:dim(xtreino)[1],replace=T)
xtreino_bag = xtreino[random_id,]
ytreino_bag = ytreino[random_id]
dados_bag = cbind(ytreino_bag,xtreino_bag)
colnames(dados_bag)[1] = "y"
reg_bag = betareg(y ~ .,
data=dados_bag,
link=linkfun)
y_pred_bag = predict(reg_bag, newdata=xteste)
matriz_bag[,preditor] = y_pred_bag
}
predicoes_bag = rowSums(matriz_bag)/n_estimadores
return(predicoes_bag)
}
timebag_inic <- Sys.time()
y_pred_bagging= bagging_betareg(xtreino=treinocadu[, -which(names(treinocadu) == "tax")],
ytreino=treinocadu[, "tax"],
xteste=testecadu[, -which(names(testecadu) == "tax")],
n_estimadores=20,
n_amostra=dim(treinocadu)[1],
linkfun = "loglog")
y_pred_bagging= bagging_betareg(xtreino=treinocadu[, -which(names(treinocadu) == "tax")],
ytreino=treinocadu[, "tax"],
xteste=testecadu[, -which(names(testecadu) == "tax")],
n_estimadores=100,
n_amostra=dim(treinocadu)[1],
linkfun = "loglog")
timebag_inic <- Sys.time()
y_pred_bagging= bagging_betareg(xtreino=treinocadu[, -which(names(treinocadu) == "tax")],
ytreino=treinocadu[, "tax"],
xteste=testecadu[, -which(names(testecadu) == "tax")],
n_estimadores=100,
n_amostra=dim(treinocadu)[1],
linkfun = "loglog")
# Marcar o final do tempo
timebag_fim <- Sys.time()
paste("Tempo de treinamento: ", timebag_fim-timebag_inic)
MAE(y=testecadu$tax,ypred=y_pred_bagging)
MSE(y=testecadu$tax,ypred=y_pred_bagging)
sqrt(MSE(y=testecadu$tax,ypred=y_pred_bagging))
1
1
1+1
if(!require(pacman)) install.packages("pacman"); library(pacman)
p_load(ggplot2,dplyr, betareg, beepr,betaboost,e1071,caret,compiler,gridExtra)
setwd("C:/Users/dioni/OneDrive - University of São Paulo/Doutorado em Estatística/2023.2/2 - Aprendizado de Máquina Estatístico/artigo_publicar/github_content/Data")
cadu = read.csv2("final_data.csv")
head(cadu)
y = cadu %>% subset(select=c(tax))
X = cadu %>% subset(select=c(ESPVIDA,FECTOT, RAZDEP,
E_ANOSESTUDO, T_ANALF18M,
T_FBBAS, T_FBFUND,
T_FBMED, T_FBSUPER,T_MED18M, T_SUPER25M,
GINI, PIND, PPOB, RDPC1, RDPCT,
THEIL, T_BANAGUA, T_DENS,
T_LIXO, T_LUZ,AGUA_ESGOTO,
T_M10A14CF, T_M15A17CF,
I_ESCOLARIDADE,  IDHM, IDHM_L, IDHM_R))
datacadu = cbind(X,y)
dim(datacadu)
## Separação entre treino e teste
set.seed(10)
# Proporção treino
prop_treino = 0.8
n_treino = round(nrow(datacadu) * prop_treino)
ind_treino = sample(1:nrow(datacadu), n_treino)
# Criar conjuntos de treino e teste
treinocadu = datacadu[ind_treino, ]
testecadu = datacadu[-ind_treino, ]
dim(treinocadu);dim(testecadu)
if(!require(pacman)) install.packages("pacman"); library(pacman)
p_load(ggplot2,dplyr, betareg, beepr,betaboost,e1071,caret,compiler,gridExtra)
## Leitura do banco de dados
setwd("C:/Users/dioni/OneDrive - University of São Paulo/Doutorado em Estatística/2023.2/2 - Aprendizado de Máquina Estatístico/artigo_publicar/github_content/Data")
cadu = read.csv2("final_data.csv")
head(cadu)
## ---
## Separação das covariaveis e da resposta
## ---
y = cadu %>% subset(select=c(tax))
X = cadu %>% subset(select=c(ESPVIDA,FECTOT, RAZDEP,
E_ANOSESTUDO, T_ANALF18M,
T_FBBAS, T_FBFUND,
T_FBMED, T_FBSUPER,T_MED18M, T_SUPER25M,
GINI, PIND, PPOB, RDPC1, RDPCT,
THEIL, T_BANAGUA, T_DENS,
T_LIXO, T_LUZ,AGUA_ESGOTO,
T_M10A14CF, T_M15A17CF,
I_ESCOLARIDADE,  IDHM, IDHM_L, IDHM_R))
## Realizando um pequena mudança na resposta
## para que o velopr não seja 1
## 1 munícipios teve todos os seus candidatos escolhidos
datacadu = cbind(X,y)
dim(datacadu)
## Separação entre treino e teste
set.seed(10)
# Proporção treino
prop_treino = 0.8
n_treino = round(nrow(datacadu) * prop_treino)
ind_treino = sample(1:nrow(datacadu), n_treino)
# Criar conjuntos de treino e teste
treinocadu = datacadu[ind_treino, ]
testecadu = datacadu[-ind_treino, ]
dim(treinocadu);dim(testecadu)
## -----
## Métricas de ajuste
## -----
MSE = function(y,ypred){mean((y - ypred)^2)}
MAE = function(y,ypred){mean(abs(y - ypred))}
R2 = function(y,ypred){
num = sum((y-ypred)^2)
dem =  sum((y-mean(y))^2)
r = 1 - (num/dem)
return(r)
}
timebreg_inic <- Sys.time()
breg = betareg(tax ~.,
data=treinocadu,
link="loglog")
timebreg_end <- Sys.time()
## Tempo de treinamento
paste("Tempo de treinamento: ", timebreg_end-timebreg_inic)
ypredbreg = predict(breg,testecadu)
MAE(y=testecadu$tax,ypred=ypredbreg)
MSE(y=testecadu$tax,ypred=ypredbreg)
sqrt(MSE(y=testecadu$tax,ypred=ypredbreg))
R2(y=testecadu$tax,ypred=ypredbreg)
bagging_betareg = function(xtreino,ytreino,xteste,n_estimadores,n_amostra,linkfun){
## predicoes do bagging em uma matriz
matriz_bag = matrix(data=0,nrow=dim(xteste)[1],ncol=n_estimadores)
enableJIT(3)
for(preditor in 1:n_estimadores){
random_id = sample(1:dim(xtreino)[1],replace=T)
xtreino_bag = xtreino[random_id,]
ytreino_bag = ytreino[random_id]
dados_bag = cbind(ytreino_bag,xtreino_bag)
colnames(dados_bag)[1] = "y"
reg_bag = betareg(y ~ .,
data=dados_bag,
link=linkfun)
y_pred_bag = predict(reg_bag, newdata=xteste)
matriz_bag[,preditor] = y_pred_bag
}
predicoes_bag = rowSums(matriz_bag)/n_estimadores
return(predicoes_bag)
}
timebag_inic <- Sys.time()
y_pred_bagging= bagging_betareg(xtreino=treinocadu[, -which(names(treinocadu) == "tax")],
ytreino=treinocadu[, "tax"],
xteste=testecadu[, -which(names(testecadu) == "tax")],
n_estimadores=100,
n_amostra=dim(treinocadu)[1],
linkfun = "loglog")
# Marcar o final do tempo
timebag_fim <- Sys.time()
paste("Tempo de treinamento: ", timebag_fim-timebag_inic)
MAE(y=testecadu$tax,ypred=y_pred_bagging)
MSE(y=testecadu$tax,ypred=y_pred_bagging)
sqrt(MSE(y=testecadu$tax,ypred=y_pred_bagging))
MAE(y=testecadu$tax,ypred=y_pred_bagging)
MSE(y=testecadu$tax,ypred=y_pred_bagging)
sqrt(MSE(y=testecadu$tax,ypred=y_pred_bagging))
timebag_inic <- Sys.time()
y_pred_bagging= bagging_betareg(xtreino=treinocadu[, -which(names(treinocadu) == "tax")],
ytreino=treinocadu[, "tax"],
xteste=testecadu[, -which(names(testecadu) == "tax")],
n_estimadores=100,
n_amostra=dim(treinocadu)[1],
linkfun = "loglog")
# Marcar o final do tempo
timebag_fim <- Sys.time()
paste("Tempo de treinamento: ", timebag_fim-timebag_inic)
MAE(y=testecadu$tax,ypred=y_pred_bagging)
MSE(y=testecadu$tax,ypred=y_pred_bagging)
sqrt(MSE(y=testecadu$tax,ypred=y_pred_bagging))
betaboost
library("devtools")
#install_github("boost-R/betaboost")
library("betaboost")
timeboost_inic <- Sys.time()
betaboosting = betaboost(tax ~.,
data=treinocadu, iterations = 500,
form.type = "betaboost")
timeboost_fim <- Sys.time()
timeboost_fim-timeboost_inic
paste("Tempo de treinamento: ", (timeboost_fim-timeboost_inic)*60)
paste("Tempo de treinamento: ", (timeboost_fim-timeboost_inic))
timeboost_fim-timeboost_inic
y_predbetaboost1 = predict(betaboosting,testecadu)
## nomalização da resposta para o intervalo (0,1)
y_predbetaboost1 = (y_predbetaboost1 - min(y_predbetaboost1))/(max(y_predbetaboost1) - min(y_predbetaboost1))
## MSE
MSE(y=testecadu$tax,ypred=y_predbetaboost1)
## RMSE
sqrt(MSE(y=testecadu$tax,ypred=y_predbetaboost1))
## MAE
MAE(y=testecadu$tax,ypred=y_predbetaboost1)
## R2: Coeficiente de determinação
R2(y=testecadu$tax,ypred=y_predbetaboost1)
n_estimators = seq(10,800,by=10)
matrix_cv_boost = matrix(data=0,nrow=length(n_estimators),ncol=1)
timecvbag_inicio <- Sys.time()
enableJIT(3)
for(i in 1:length(n_estimators)){
cv_bestaboost = betaboost(tax ~.,
data=treinocadu, iterations = n_estimators[i],
form.type = "betaboost")
y_pred_boost = predict(cv_bestaboost,testecadu)
matrix_cv_boost[i,1] = MSE(y=testecadu$tax,ypred=y_pred_boost)
print(paste("estimators: ", n_estimators[i]))
}
head(matrix_cv_boost)
plot(matrix_cv_boost,type='b')
plot(matrix_cv_boost,type='b')
cv_boost = as.data.frame(matrix_cv_boost)
cv_boost
cv_boost[1:5,]
head(cv_boost)
cv_bag$ests = n_estimators
cv_boost = as.data.frame(matrix_cv_boost)
cv_boost$ests = n_estimators
colnames(cv_boost)[1] = "EQM"
head(cv_boost)
ggplot(data=cv_boost, aes(x=n_estimators, y=EQM, group=1)) +
geom_line(linetype = "dashed")+
geom_point(color='royalblue4',size=2) +
xlab("\n Number of estimators") +
ylab("MSE \n")+
theme_minimal()
## Cross-validation Process in Boosting for Beta Regression
n_estimators = seq(10,800,by=10)
matrix_cv_boost = matrix(data=0,nrow=length(n_estimators),ncol=1)
timecvbag_inicio <- Sys.time()
enableJIT(3)
for(i in 1:length(n_estimators)){
cv_bestaboost = betaboost(tax ~.,
data=treinocadu, iterations = n_estimators[i],
form.type = "betaboost")
y_pred_boost = predict(cv_bestaboost,testecadu)
matrix_cv_boost[i,1] = MSE(y=testecadu$tax,ypred=y_pred_boost)
print(paste("estimators: ", n_estimators[i]))
}
## Tempo para a validação cruzada
timecvboost_fim-timecvboost_inicio
timecvboost_fim <- Sys.time()
## Tempo para a validação cruzada
timecvboost_fim-timecvboost_inicio
timecvboost_inicio <- Sys.time()
timecvboost_fim <- Sys.time()
## Tempo para a validação cruzada
timecvboost_fim-timecvboost_inicio
## Cross-validation Process in Boosting for Beta Regression
n_estimators = seq(10,800,by=10)
matrix_cv_boost = matrix(data=0,nrow=length(n_estimators),ncol=1)
timecvboost_inicio <- Sys.time()
enableJIT(3)
for(i in 1:length(n_estimators)){
cv_bestaboost = betaboost(tax ~.,
data=treinocadu, iterations = n_estimators[i],
form.type = "betaboost")
y_pred_boost = predict(cv_bestaboost,testecadu)
matrix_cv_boost[i,1] = MSE(y=testecadu$tax,ypred=y_pred_boost)
print(paste("estimators: ", n_estimators[i]))
}
timecvboost_fim <- Sys.time()
cv_boost = as.data.frame(matrix_cv_boost)
cv_boost$ests = n_estimators
colnames(cv_boost)[1] = "EQM"
ggplot(data=cv_boost, aes(x=n_estimators, y=EQM, group=1)) +
geom_line(linetype = "dashed")+
geom_point(color='royalblue4',size=2) +
xlab("\n Number of estimators") +
ylab("MSE \n")+
theme_minimal()
## Tempo para a validação cruzada
timecvboost_fim-timecvboost_inicio
## Tempo para a validação cruzada
timecvboost_fim-timecvboost_inicio
y_pred_boost
hist(y_pred_boost)
